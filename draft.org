#+TITLE: Modelling Vowel Harmony with Neural Networks
#+AUTHOR: Irfan S
#+latex_class: article-no-defaults
#+OPTIONS: toc:nil num:t noescape:t defaults:nil
#+latex_header: \usepackage{mathspec}
# #+latex_header: \usepackage{fullpage}
# #+latex_header: \usepackage[notipa,shadedcells]{ot-tableau}
#+latex_header: \usepackage{natbib}
#+latex_header: \usepackage{setspace}
#+latex_header: \setmainfont{Times New Roman}

\doublespacing
\maketitle

* Set-up
** Vowel Harmony
Vowel harmony (henceforth VH) is a long-distance assimilatory process involving vowels within a certain morphological or prosodic domain. Long-distance here means that that the assimilatory effect affects vowels that are not produced in contiguity, and may have one or more intervening segments. cite:anderson1980 presents a discussion of examples of VH attested in various languages with a view to arriving at a unified formal definition for the process. Acknowledging the variety with which VH is realised in different languages, citeauthor:anderson1980 finally concludes that the formal mechanism of VH is not distinct from that of other forms of metaphony, such as dissimilation or Umlaut rules, stating that "this means there is nothing special about vowel harmony". With specific reference to the phonetic motivatedness for VH, citeauthor:anderson1980, while acknowledging that such motivations are apparent in the historical basis of VH, says that phonetic grounding cannot be considered a necessary condition for vowel harmony systems from a synchronic point of view.

** Coarticulatory approaches to Vowel Harmony
A different approach is proposed by cite:ohala1994towards in the form of a universal and phonetically-based theory of VH. According to citeauthor:ohala1994towards, VH is the "fossilised result of purely phonetic between-vowel assimilations", the result of sound change that occurs when two independent production events (such as a vowel and the perturbation caused in the production of that vowel by a neighbouring vowel, in the case of VH) are misperceived as one by the listener. If this happens repeatedly over time and with many speaker-listener pairs, it could cause a new pronunciation norm to emerge that would be integrated into the language's grammar. In this way perturbatory phonetic effects, or coarticulatory effects, of a unit of speech on neighbouring units could be linked to phonological processes such as harmony. An inconclusive attempt to find experimental evidence in support of this was made by cite:busa1999search. Stronger supporting evidence was found by cite:przezdziecki2005vowel, who compared three different dialects of Yoruba, one attesting a VH process and two without. It was found that the effect and context of the high vowel harmony pattern in VH-attesting dialect was similar to the coarticulatory behaviour observed for the corresponding vowels in the other two dialects. Further, a decision-tree learner model applied to the data by citeauthor:przezdziecki2005vowel resulted in splits corresponding to the natural classes involved in the harmony process (±ATR), even when unconstrained by featural specifications. In the author's words, "pre-existing abstract features were not necessary for the learner to extract a phonological pattern from a coarticulatory pattern" (see citealp:nevins2010locality for a rejoinder asserting that VH and coarticulation are entirely distinct processes). 

** Temporal Domain of coarticulation
One of the questions raised against linking VH and coarticulation is based on the assumption that grounding VH in a phonetic process such as coarticulation would imply strict locality citep:kimper2017not, while VH rules can apply over long distances, across syllable boundaries and even word boundaries in some cases. cite:grosvald2009interspeaker shows that vowel-to-vowel coarticulatory effects can even be detected across at least 3-vowel distances in productions by speakers of American English, a non-VH language. Interestingly, citeauthor:grosvald2009interspeaker also found that these coarticulatory effects could also be perceived across similar spans, while noting considerable inter-subject variability in both the production and perception tasks. Using nonce stimuli in a perception task setting, cite:kimper2017not shows that vowel harmony is perceptually advantageous even for non-adjacent vowels.

** Exemplar modelling approaches to Vowel Harmony
cite:cole2009emergent introduces an alternative to the representational models of generative phonology in keeping with the evolutionary, phonetically grounded approach advocated by cite:blevins2004evolutionary and cite:ohala1994towards. She proposes an exemplar-theoretic model, with no a priori feature representations imposed as inputs. Instead, she makes the claim that the feature structure required for a phonological account of harmony would be an emergent property of the exemplar model's encoding. citeauthor:cole2009emergent points to the Simple Recurrent Network model demonstrated by McClelland and Elman, 1986 (cited in citealp:cole2009emergent), as a suitable way to implement the exemplar model, which would essentially model transitional probabilities between input elements of speech.

cite:mailhot2010instance demonstrates LIʙPʜᴏɴ\footnote{the Lazy Instance-based Phonologist}, an implementation of an exemplar-based learning algorithm. The input /instances/ to the model are acoustic features corresponding to word-level units. The model implements a perception (instance to label) and a production module (label to stored representation of instance). The reported results indicate that the model is able to generalise and produce harmonically correct morphological forms that it was not exposed to during training. cite:mailhot2010instance provides a proof-of-concept implementation of the modelling approach suggested by cite:cole2009emergent, even though this implementation directly produces a word-label and not phone transitional probabilities.



* The proposed model
I propose that a Long Short Term Memory (LSTM) network of sufficient architecture, a modified kind of recurrent neural network, be used to implement an encoder-decoder model. The encoder would be trained to generate transition probabilities for a sequence of utterance level acoustic features. The decoder will be trained to regenerate those sequences from the probabilities output by the encoder. In tandem, the encoder-decoder would approximate a perception/production model of an individual speaker.

#+BEGIN_SRC python
# Model class (using Keras)
#+END_SRC


** Data
Large corpora of natural speech recordings from multiple speakers of both harmony and non-harmony languages would be required. A large enough corpus would ideally be representative of the language community.

#+BEGIN_SRC python
# Data streamer class
#+END_SRC

** Experiments

Experiments would involve training the encoder/decoder model on instances of utterance level acoustic features. The variance in the input could be controlled to simulate various "settings" of the language community, which should result the model in learning differently. The outputs produced by the model could be tested to see if featural structures required for harmony (or other phonological processes) emerge, as predicted by cite:cole2009emergent.


bibliography:draft.bib
bibliographystyle:unsrtnat
